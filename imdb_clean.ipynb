{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean code for IMBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Convert ratings data\n",
    "\n",
    "Format should be user_id, item_id, rating\n",
    "User_id and item_id should be consecutive\n",
    "(for the sparse matrix to work)\n",
    "The __init__ should have dictionaries to go from consecutive id to real id\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "num_interactions = 100_000\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM EVIC.ratings\n",
    "WHERE rating > 3.0\n",
    "LIMIT {num_interactions}\n",
    "\"\"\"\n",
    "ratings = pd.read_gbq(query, project_id=\"spike-sandbox\",\n",
    "                      use_bqstorage_api=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2771, 7017)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_users = ratings.user_id.unique()\n",
    "n_unique_users = len(unique_users)\n",
    "unique_movies = ratings.movie_id.unique()\n",
    "n_unique_movies = len(unique_movies)\n",
    "\n",
    "n_unique_users, n_unique_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_orig_id_to_consecutive_id_dict = {key:value \n",
    "                                       for key, value\n",
    "                            in zip(unique_users, range(1, n_unique_users+1))}\n",
    "\n",
    "movie_orig_id_to_consecutive_id_dict = {key:value \n",
    "                                       for key, value\n",
    "                            in zip(unique_movies, range(1, n_unique_movies+1))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "consecutive_user_id = np.empty(len(ratings), dtype=np.int)\n",
    "\n",
    "for j, orig_id in enumerate(ratings.user_id.values):\n",
    "    consecutive_user_id[j] = user_orig_id_to_consecutive_id_dict[orig_id]\n",
    "\n",
    "ratings['consecutive_user_id'] = consecutive_user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "consecutive_movie_id = np.empty(len(ratings), dtype=np.int)\n",
    "\n",
    "for j, orig_id in enumerate(ratings.movie_id.values):\n",
    "    consecutive_movie_id[j] = movie_orig_id_to_consecutive_id_dict[orig_id]\n",
    "\n",
    "ratings['consecutive_movie_id'] = consecutive_movie_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Warm start test set: erase random 10% of interactions\n",
    "test_pct = 0.1\n",
    "ratings['test'] = (np.random.random_sample(size=len(ratings)) > (1 - test_pct))*1\n",
    "ratings['rating'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output: evic.test.rating, evic.train.rating\n",
    "columns = ['consecutive_user_id', 'consecutive_movie_id', 'rating']\n",
    "ratings.query(\"test == 1\")[columns].to_csv(\"data/evic.test.rating\",\n",
    "                                           index=False, sep='\\t', header=False)\n",
    "ratings.query(\"test == 0\")[columns].to_csv(\"data/evic.train.rating\",\n",
    "                                           index=False, sep='\\t', header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Python imports\n",
    "import argparse\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Workspace imports\n",
    "from src.evaluate import evaluate_model\n",
    "from src.Dataset import MovieLensDataset\n",
    "from src.utils import train_one_epoch, test, plot_statistics\n",
    "\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_args_dict = {\n",
    "    \"path\": \"data/\",\n",
    "    \"dataset\": \"evic\", # :-)\n",
    "    \"epochs\": 18,\n",
    "    \"batch_size\": 256,\n",
    "    \"layers\": [16, 32, 16, 8],\n",
    "    'weight_decay': 0.00001,\n",
    "    \"num_neg_train\": 4, #'Number of negative instances to pair \n",
    "                        #with a positive instance while training'\n",
    "    \"num_neg_test\": 100,\n",
    "    \"lr\": 0.001,\n",
    "    \"dropout\": 0.,\n",
    "    \"learner\": \"adam\",\n",
    "    \"verbose\": 1,\n",
    "    \"out\": 1 #save trained model or not\n",
    "}\n",
    "\n",
    "args = default_args_dict\n",
    "path = args[\"path\"]\n",
    "dataset = args[\"dataset\"]\n",
    "layers = args[\"layers\"]\n",
    "weight_decay = args[\"weight_decay\"]\n",
    "num_negatives_train = args[\"num_neg_train\"]\n",
    "num_negatives_test = args[\"num_neg_test\"]\n",
    "dropout = args[\"dropout\"]\n",
    "learner = args[\"learner\"]\n",
    "learning_rate = args[\"lr\"]\n",
    "batch_size = args[\"batch_size\"]\n",
    "epochs = args[\"epochs\"]\n",
    "verbose = args[\"verbose\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data done [11.5 s]. #user=2772, #item=7018, #train=90059, #test=9941\n"
     ]
    }
   ],
   "source": [
    "topK = 10\n",
    "t1 = time()\n",
    "full_dataset = MovieLensDataset(\n",
    "    path + dataset, num_negatives_train=num_negatives_train,\n",
    "    num_negatives_test=num_negatives_test)\n",
    "train, testRatings, testNegatives = (full_dataset.trainMatrix,\n",
    "                                     full_dataset.testRatings,\n",
    "                                     full_dataset.testNegatives)\n",
    "num_users, num_items = train.shape\n",
    "print(\"Load data done [%.1f s]. #user=%d, #item=%d, #train=%d, #test=%d\"\n",
    "      % (time()-t1, num_users, num_items, train.nnz, len(testRatings)))\n",
    "\n",
    "training_data_generator = DataLoader(\n",
    "    full_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, n_users, n_items, layers=[16, 8], dropout=False):\n",
    "        \"\"\"\n",
    "        Simple Feedforward network with Embeddings for users and items\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert (layers[0] % 2 == 0), \"layers[0] must be an even number\"\n",
    "        self.__alias__ = \"MLP {}\".format(layers)\n",
    "        self.__dropout__ = dropout\n",
    "\n",
    "        # user and item embedding layers\n",
    "        embedding_dim = int(layers[0]/2)\n",
    "        self.user_embedding = torch.nn.Embedding(n_users, embedding_dim)\n",
    "        self.item_embedding = torch.nn.Embedding(n_items, embedding_dim)\n",
    "\n",
    "        # list of weight matrices\n",
    "        self.fc_layers = torch.nn.ModuleList()\n",
    "        # hidden dense layers\n",
    "        for _, (in_size, out_size) in enumerate(zip(layers[:-1], layers[1:])):\n",
    "            self.fc_layers.append(torch.nn.Linear(in_size, out_size))\n",
    "        # final prediction layer\n",
    "        self.output_layer = torch.nn.Linear(layers[-1], 1)\n",
    "\n",
    "    def forward(self, feed_dict):\n",
    "        users = feed_dict['user_id']\n",
    "        items = feed_dict['item_id']\n",
    "        user_embedding = self.user_embedding(users)\n",
    "        item_embedding = self.item_embedding(items)\n",
    "        # concatenate user and item embeddings to form input\n",
    "        x = torch.cat([user_embedding, item_embedding], 1)\n",
    "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
    "            x = self.fc_layers[idx](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x,  p=self.__dropout__, training=self.training)\n",
    "        logit = self.output_layer(x)\n",
    "        rating = torch.sigmoid(logit)\n",
    "        return rating\n",
    "\n",
    "    def predict(self, feed_dict):\n",
    "        # return the score, inputs and outputs are numpy arrays\n",
    "        for key in feed_dict:\n",
    "            if type(feed_dict[key]) != type(None):\n",
    "                feed_dict[key] = torch.from_numpy(\n",
    "                    feed_dict[key]).to(dtype=torch.long, device=device)\n",
    "        output_scores = self.forward(feed_dict)\n",
    "        return output_scores.cpu().detach().numpy()\n",
    "\n",
    "    def get_alias(self):\n",
    "        return self.__alias__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (user_embedding): Embedding(2772, 8)\n",
      "  (item_embedding): Embedding(7018, 8)\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (2): Linear(in_features=16, out_features=8, bias=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MLP(num_users, num_items, layers=layers, dropout=dropout)\n",
    "model.to(device)\n",
    "if verbose:\n",
    "    print(model)\n",
    "    \n",
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), weight_decay=weight_decay)\n",
    "\n",
    "# Record performance\n",
    "hr_list = []\n",
    "ndcg_list = []\n",
    "BCE_loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval: HR = 0.1010, NDCG = 0.0470 [8.9 s]\n",
      "Epoch = 0\n",
      "Epoch completed 6.2 s\n",
      "Train Loss: 0.4724575046485903\n",
      "doing epoch 0\n",
      "Eval: HR = 0.4915, NDCG = 0.2594 [8.5 s]\n",
      "Epoch = 1\n",
      "Epoch completed 6.3 s\n",
      "Train Loss: 0.373765311114418\n",
      "doing epoch 1\n",
      "Eval: HR = 0.5677, NDCG = 0.3115 [8.6 s]\n",
      "Epoch = 2\n",
      "Epoch completed 6.2 s\n",
      "Train Loss: 0.3505717394550528\n",
      "doing epoch 2\n",
      "Eval: HR = 0.5753, NDCG = 0.3177 [8.5 s]\n",
      "Epoch = 3\n",
      "Epoch completed 6.2 s\n",
      "Train Loss: 0.3420019286677809\n",
      "doing epoch 3\n",
      "Eval: HR = 0.5758, NDCG = 0.3171 [8.6 s]\n",
      "Epoch = 4\n",
      "Epoch completed 6.2 s\n",
      "Train Loss: 0.33797582275763094\n",
      "doing epoch 4\n",
      "Eval: HR = 0.5765, NDCG = 0.3175 [8.5 s]\n",
      "Epoch = 5\n",
      "Epoch completed 6.1 s\n",
      "Train Loss: 0.3354666643670637\n",
      "doing epoch 5\n",
      "Eval: HR = 0.5775, NDCG = 0.3178 [8.6 s]\n",
      "Epoch = 6\n",
      "Epoch completed 6.2 s\n",
      "Train Loss: 0.33374034579061795\n",
      "doing epoch 6\n",
      "Eval: HR = 0.5750, NDCG = 0.3170 [8.5 s]\n",
      "Epoch = 7\n",
      "Epoch completed 6.1 s\n",
      "Train Loss: 0.33213773711185823\n",
      "doing epoch 7\n",
      "Eval: HR = 0.5737, NDCG = 0.3168 [8.5 s]\n",
      "Epoch = 8\n",
      "Epoch completed 6.2 s\n",
      "Train Loss: 0.33039561218162233\n",
      "doing epoch 8\n",
      "Eval: HR = 0.5718, NDCG = 0.3155 [8.4 s]\n",
      "Epoch = 9\n",
      "Epoch completed 6.0 s\n",
      "Train Loss: 0.3282326054613722\n",
      "doing epoch 9\n",
      "Eval: HR = 0.5719, NDCG = 0.3150 [8.3 s]\n",
      "Epoch = 10\n",
      "Epoch completed 6.1 s\n",
      "Train Loss: 0.32559337641042085\n",
      "doing epoch 10\n",
      "Eval: HR = 0.5671, NDCG = 0.3142 [8.6 s]\n",
      "Epoch = 11\n",
      "Epoch completed 6.5 s\n",
      "Train Loss: 0.3226649827276732\n",
      "doing epoch 11\n",
      "Eval: HR = 0.5698, NDCG = 0.3141 [8.6 s]\n",
      "Epoch = 12\n",
      "Epoch completed 6.2 s\n",
      "Train Loss: 0.31902143702275243\n",
      "doing epoch 12\n",
      "Eval: HR = 0.5655, NDCG = 0.3117 [8.6 s]\n",
      "Epoch = 13\n",
      "Epoch completed 6.1 s\n",
      "Train Loss: 0.31519468620868485\n",
      "doing epoch 13\n",
      "Eval: HR = 0.5614, NDCG = 0.3080 [8.6 s]\n",
      "Epoch = 14\n",
      "Epoch completed 6.4 s\n",
      "Train Loss: 0.3112526012202296\n",
      "doing epoch 14\n",
      "Eval: HR = 0.5619, NDCG = 0.3088 [8.6 s]\n",
      "Epoch = 15\n",
      "Epoch completed 6.4 s\n",
      "Train Loss: 0.30747753572335224\n",
      "doing epoch 15\n",
      "Eval: HR = 0.5596, NDCG = 0.3063 [8.6 s]\n",
      "Epoch = 16\n",
      "Epoch completed 6.4 s\n",
      "Train Loss: 0.3035279626514913\n",
      "doing epoch 16\n",
      "Eval: HR = 0.5583, NDCG = 0.3060 [8.7 s]\n",
      "Epoch = 17\n",
      "Epoch completed 6.4 s\n",
      "Train Loss: 0.3000247973976249\n",
      "doing epoch 17\n",
      "Eval: HR = 0.5541, NDCG = 0.3038 [8.7 s]\n",
      "Epoch = 18\n",
      "Epoch completed 6.4 s\n",
      "Train Loss: 0.2966124976287856\n",
      "doing epoch 18\n",
      "Eval: HR = 0.5515, NDCG = 0.3009 [8.7 s]\n",
      "Epoch = 19\n",
      "Epoch completed 6.4 s\n",
      "Train Loss: 0.2936680627147063\n",
      "doing epoch 19\n",
      "Eval: HR = 0.5501, NDCG = 0.2994 [8.7 s]\n",
      "Epoch = 20\n",
      "Epoch completed 6.4 s\n",
      "Train Loss: 0.2909131327886484\n",
      "doing epoch 20\n",
      "Eval: HR = 0.5478, NDCG = 0.2986 [8.7 s]\n",
      "Epoch = 21\n",
      "Epoch completed 6.4 s\n",
      "Train Loss: 0.2883447077783961\n",
      "doing epoch 21\n",
      "Eval: HR = 0.5481, NDCG = 0.2982 [8.7 s]\n",
      "Epoch = 22\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-8f123735cc7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     epoch_loss = train_one_epoch(model, training_data_generator,\n\u001b[0;32m---> 10\u001b[0;31m                                  loss_fn, optimizer, epoch, device)\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"doing epoch {epoch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/encodings_for_cold_start/src/utils.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, data_loader, loss_fn, optimizer, epoch_no, device, verbose)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;31m# accumulate the loss for monitoring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mepoch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/spike_basicoV5/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Check Init performance\n",
    "hr, ndcg = test(model, full_dataset, topK)\n",
    "hr_list.append(hr)\n",
    "ndcg_list.append(ndcg)\n",
    "BCE_loss_list.append(1)\n",
    "# do the epochs now\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = train_one_epoch(model, training_data_generator,\n",
    "                                 loss_fn, optimizer, epoch, device)\n",
    "\n",
    "    if epoch % verbose == 0:\n",
    "        hr, ndcg = test(model, full_dataset, topK)\n",
    "        hr_list.append(hr)\n",
    "        ndcg_list.append(ndcg)\n",
    "        BCE_loss_list.append(epoch_loss)\n",
    "        # if hr > best_hr:\n",
    "        #     best_hr, best_ndcg, best_iter = hr, ndcg, epoch\n",
    "        #     if args.out > 0:\n",
    "        #         model.save(model_out_file, overwrite=True)\n",
    "print(\"hr for epochs: \", hr_list)\n",
    "print(\"ndcg for epochs: \", ndcg_list)\n",
    "print(\"loss for epochs: \", BCE_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 % verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spike_basicoV5]",
   "language": "python",
   "name": "conda-env-spike_basicoV5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
