{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean code for IMBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Convert ratings data\n",
    "\n",
    "Format should be user_id, item_id, rating\n",
    "User_id and item_id should be consecutive\n",
    "(for the sparse matrix to work)\n",
    "The __init__ should have dictionaries to go from consecutive id to real id\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "num_interactions = 100_000\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM EVIC.ratings\n",
    "WHERE rating > 3.0\n",
    "LIMIT {num_interactions}\n",
    "\"\"\"\n",
    "ratings = pd.read_gbq(query, project_id=\"spike-sandbox\",\n",
    "                      use_bqstorage_api=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2771, 7017)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_users = ratings.user_id.unique()\n",
    "n_unique_users = len(unique_users)\n",
    "unique_movies = ratings.movie_id.unique()\n",
    "n_unique_movies = len(unique_movies)\n",
    "\n",
    "n_unique_users, n_unique_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_orig_id_to_consecutive_id_dict = {key:value \n",
    "                                       for key, value\n",
    "                            in zip(unique_users, range(1, n_unique_users+1))}\n",
    "\n",
    "movie_orig_id_to_consecutive_id_dict = {key:value \n",
    "                                       for key, value\n",
    "                            in zip(unique_movies, range(1, n_unique_movies+1))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "consecutive_user_id = np.empty(len(ratings), dtype=np.int)\n",
    "\n",
    "for j, orig_id in enumerate(ratings.user_id.values):\n",
    "    consecutive_user_id[j] = user_orig_id_to_consecutive_id_dict[orig_id]\n",
    "\n",
    "ratings['consecutive_user_id'] = consecutive_user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "consecutive_movie_id = np.empty(len(ratings), dtype=np.int)\n",
    "\n",
    "for j, orig_id in enumerate(ratings.movie_id.values):\n",
    "    consecutive_movie_id[j] = movie_orig_id_to_consecutive_id_dict[orig_id]\n",
    "\n",
    "ratings['consecutive_movie_id'] = consecutive_movie_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Warm start test set: erase random 10% of interactions\n",
    "test_pct = 0.1\n",
    "ratings['test'] = (np.random.random_sample(size=len(ratings)) > (1 - test_pct))*1\n",
    "ratings['rating'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output: evic.test.rating, evic.train.rating\n",
    "columns = ['consecutive_user_id', 'consecutive_movie_id', 'rating']\n",
    "ratings.query(\"test == 1\")[columns].to_csv(\"data/evic.test.rating\",\n",
    "                                           index=False, sep='\\t', header=False)\n",
    "ratings.query(\"test == 0\")[columns].to_csv(\"data/evic.train.rating\",\n",
    "                                           index=False, sep='\\t', header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Python imports\n",
    "import argparse\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Workspace imports\n",
    "from src.evaluate import evaluate_model\n",
    "from src.Dataset import MovieLensDataset\n",
    "from src.utils import train_one_epoch, test, plot_statistics\n",
    "\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_args_dict = {\n",
    "    \"path\": \"data/\",\n",
    "    \"dataset\": \"evic\", # :-)\n",
    "    \"epochs\": 18,\n",
    "    \"batch_size\": 256,\n",
    "    \"layers\": [16, 32, 16, 8],\n",
    "    'weight_decay': 0.00001,\n",
    "    \"num_neg_train\": 4, #'Number of negative instances to pair \n",
    "                        #with a positive instance while training'\n",
    "    \"num_neg_test\": 100,\n",
    "    \"lr\": 0.001,\n",
    "    \"dropout\": 0.,\n",
    "    \"learner\": \"adam\",\n",
    "    \"verbose\": 1,\n",
    "    \"out\": 1 #save trained model or not\n",
    "}\n",
    "\n",
    "args = default_args_dict\n",
    "path = args[\"path\"]\n",
    "dataset = args[\"dataset\"]\n",
    "layers = args[\"layers\"]\n",
    "weight_decay = args[\"weight_decay\"]\n",
    "num_negatives_train = args[\"num_neg_train\"]\n",
    "num_negatives_test = args[\"num_neg_test\"]\n",
    "dropout = args[\"dropout\"]\n",
    "learner = args[\"learner\"]\n",
    "learning_rate = args[\"lr\"]\n",
    "batch_size = args[\"batch_size\"]\n",
    "epochs = args[\"epochs\"]\n",
    "verbose = args[\"verbose\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data done [10.9 s]. #user=2772, #item=7018, #train=90059, #test=9941\n"
     ]
    }
   ],
   "source": [
    "topK = 10\n",
    "t1 = time()\n",
    "full_dataset = MovieLensDataset(\n",
    "    path + dataset, num_negatives_train=num_negatives_train,\n",
    "    num_negatives_test=num_negatives_test)\n",
    "train, testRatings, testNegatives = (full_dataset.trainMatrix,\n",
    "                                     full_dataset.testRatings,\n",
    "                                     full_dataset.testNegatives)\n",
    "num_users, num_items = train.shape\n",
    "print(\"Load data done [%.1f s]. #user=%d, #item=%d, #train=%d, #test=%d\"\n",
    "      % (time()-t1, num_users, num_items, train.nnz, len(testRatings)))\n",
    "\n",
    "training_data_generator = DataLoader(\n",
    "    full_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, n_users, n_items, layers=[16, 8], dropout=False):\n",
    "        \"\"\"\n",
    "        Simple Feedforward network with Embeddings for users and items\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert (layers[0] % 2 == 0), \"layers[0] must be an even number\"\n",
    "        self.__alias__ = \"MLP {}\".format(layers)\n",
    "        self.__dropout__ = dropout\n",
    "\n",
    "        # user and item embedding layers\n",
    "        embedding_dim = int(layers[0]/2)\n",
    "        self.user_embedding = torch.nn.Embedding(n_users, embedding_dim)\n",
    "        self.item_embedding = torch.nn.Embedding(n_items, embedding_dim)\n",
    "\n",
    "        # list of weight matrices\n",
    "        self.fc_layers = torch.nn.ModuleList()\n",
    "        # hidden dense layers\n",
    "        for _, (in_size, out_size) in enumerate(zip(layers[:-1], layers[1:])):\n",
    "            self.fc_layers.append(torch.nn.Linear(in_size, out_size))\n",
    "        # final prediction layer\n",
    "        self.output_layer = torch.nn.Linear(layers[-1], 1)\n",
    "\n",
    "    def forward(self, feed_dict):\n",
    "        users = feed_dict['user_id']\n",
    "        items = feed_dict['item_id']\n",
    "        user_embedding = self.user_embedding(users)\n",
    "        item_embedding = self.item_embedding(items)\n",
    "        # concatenate user and item embeddings to form input\n",
    "        x = torch.cat([user_embedding, item_embedding], 1)\n",
    "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
    "            x = self.fc_layers[idx](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x,  p=self.__dropout__, training=self.training)\n",
    "        logit = self.output_layer(x)\n",
    "        rating = torch.sigmoid(logit)\n",
    "        return rating\n",
    "\n",
    "    def predict(self, feed_dict):\n",
    "        # return the score, inputs and outputs are numpy arrays\n",
    "        for key in feed_dict:\n",
    "            if type(feed_dict[key]) != type(None):\n",
    "                feed_dict[key] = torch.from_numpy(\n",
    "                    feed_dict[key]).to(dtype=torch.long, device=device)\n",
    "        output_scores = self.forward(feed_dict)\n",
    "        return output_scores.cpu().detach().numpy()\n",
    "\n",
    "    def get_alias(self):\n",
    "        return self.__alias__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (user_embedding): Embedding(2772, 8)\n",
      "  (item_embedding): Embedding(7018, 8)\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (2): Linear(in_features=16, out_features=8, bias=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MLP(num_users, num_items, layers=layers, dropout=dropout)\n",
    "model.to(device)\n",
    "if verbose:\n",
    "    print(model)\n",
    "    \n",
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), weight_decay=weight_decay)\n",
    "\n",
    "# Record performance\n",
    "hr_list = []\n",
    "ndcg_list = []\n",
    "BCE_loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "data_loader = training_data_generator\n",
    "for feed_dict in data_loader:\n",
    "    for key in feed_dict:\n",
    "        if type(feed_dict[key]) != type(None):\n",
    "            feed_dict[key] = feed_dict[key].to(dtype = torch.long, device = device)\n",
    "    # get the predictions\n",
    "    prediction = model(feed_dict)\n",
    "    # print(prediction.shape)\n",
    "    # get the actual targets\n",
    "    rating = feed_dict['rating']\n",
    "\n",
    "\n",
    "    # convert to float and change dim from [batch_size] to [batch_size,1]\n",
    "    rating = rating.float().view(prediction.size())  \n",
    "    loss = loss_fn(prediction, rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1759"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, feed_dict in enumerate(data_loader):    \n",
    "    if i == 0:\n",
    "        feed_dict_1 = feed_dict\n",
    "    elif i == 3:\n",
    "        feed_dict_2 = feed_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': tensor([1875,  931,  535, 1948, 1957,  555,   13, 2127, 1461, 2510, 2555, 1615,\n",
       "         1783, 2699,  974,   81, 1695, 1298, 2252, 1782, 1354,  412,  124, 2276,\n",
       "         2378, 1834, 2000, 1120, 2488, 2349, 2052,  326, 2255,  313, 1101,   36,\n",
       "         2424, 1433, 1410, 1139,  791, 1370, 1549,  358, 1587, 1399, 1763,  848,\n",
       "          939, 2284, 2270, 1757, 2626, 1782, 1615, 1116, 1909, 2055, 1689,  822,\n",
       "          118,   36, 1956,  224, 1228,    3,   34,  961, 2207,   11,  746, 1564,\n",
       "         1117, 2647,  632,  118,  505, 1625,  371,  498,  174, 2468, 1668,  262,\n",
       "         1121,  298,  103, 2116, 1292, 1708, 2603,  180, 1961, 2484,  331, 1446,\n",
       "         2199, 2139, 1767,  974,  537, 2598, 2651, 1349, 2311, 2100, 1121, 1397,\n",
       "         1121,  706, 1446, 2336, 1247,  326, 1535, 1202, 2046, 2728, 1980,  166,\n",
       "         1436,  188, 2063, 1640,  969,  886, 2081, 1469, 1533,  627,  678, 2674,\n",
       "          784, 1117, 2125, 2445, 1370, 1899,   68, 2100, 1555, 1177,  910, 2508,\n",
       "         2608, 1639,  212,  346,  538, 1755, 1980, 2660,   89,  672,  322,  537,\n",
       "         2769, 1040, 1495, 2117,   79,  877, 2598,  976,  926,  897,   74, 1533,\n",
       "         2423,  881, 1715,   56,  457,  390, 2758, 2414, 2553,  671, 1902,  394,\n",
       "         2718,  754, 2456,  515, 1956, 1362, 2267, 1181,   20, 2197, 1698,  695,\n",
       "         1081,  458, 2087, 1477, 2051, 2000,  155, 1731,  136,   47, 1146,   68,\n",
       "          513, 1661,  730, 1701, 2078, 2377, 2749, 2120, 2483, 2660, 1500, 1058,\n",
       "         1564,  771, 1401,  884,  585, 2162, 1890,  298, 1909,   36,  725, 1037,\n",
       "         1888, 2543, 2618,  767,  214, 2660,  317,  279,  635, 1901, 1726,  473,\n",
       "         2730,  969, 2650, 2252, 1244, 1601,  257,  256, 1963,  330,  353,  152,\n",
       "         2051,  405,   21,  892]),\n",
       " 'item_id': tensor([1302, 4109, 2636, 2791, 6454, 3773, 2670, 6902,  242,  786, 1051, 4034,\n",
       "         3463,   58, 5783, 1489, 3992,   19,  642, 4202,  488, 2971, 5421, 4703,\n",
       "         6812,  289, 1217, 1212, 6558, 1352, 1664, 2860, 1035, 2500, 1316, 3673,\n",
       "         5141, 6242, 3767,  472, 2153, 5610, 2713, 5903, 6460, 5109, 3004, 6148,\n",
       "         1121, 1628, 5702,  844,  647, 4965, 2969,  237, 6502, 5057,  358,  988,\n",
       "         3567, 2275, 4968, 1253, 3445, 6639, 4392, 1497, 3558, 6745, 4062, 2685,\n",
       "         6055, 4991, 4242, 5069, 5407, 2870, 5907,  528, 1589,  656,  983, 6335,\n",
       "         3963, 3849, 4089, 3603,   87, 4050, 2283, 1005, 3857, 4196,   81,  616,\n",
       "         6648, 3067, 3672, 4839, 6535, 1501,  540, 5471, 3789,  248, 5444, 3579,\n",
       "         1180,  884, 2635, 1212, 4136, 3033, 1830, 4086,   86, 4969, 4181, 3513,\n",
       "          513, 5065, 3679, 3607, 3000, 6667, 4000, 2316,   93, 2638, 2013, 6985,\n",
       "         1624, 6599, 6982, 2792, 4432, 5985, 4055, 5046, 5671, 3937, 1908, 4605,\n",
       "         5017, 3462, 7004, 5612,  262, 4895, 5478, 1056, 2134, 6916, 1903, 1724,\n",
       "         3145,  543, 1200,  740, 5004, 1060, 2056, 6354, 3892, 1601, 1080, 4026,\n",
       "         3544, 6008,  937, 1120, 6885, 4607,  135, 6582, 4636, 6354,  742, 1696,\n",
       "          238, 1639,  858, 2161,  664, 3234, 3036, 6424, 1302, 2430, 1486,  652,\n",
       "         5263, 1110, 4023, 1387, 5458, 5063, 6872, 2906, 1466, 5022,  655,  317,\n",
       "         5677, 1984, 3530,  336, 3698, 1477, 1588, 4525, 3992, 2776, 3102, 2333,\n",
       "         6286,  841, 2724, 2952, 3275, 4670, 5229, 3939, 5622,  719, 1626, 1462,\n",
       "         4547, 1809,  890,  295, 2091, 6260, 1500, 1644,  585,  765, 3385, 6264,\n",
       "         4825, 3847,  392, 6230, 3361, 4969, 2789, 2098, 4696, 3105, 1883, 1514,\n",
       "         2343,  180, 1780,  304]),\n",
       " 'rating': tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "         0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "         0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "         0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_dict_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1347, 1043,  467, 2662, 2370, 1779, 2556,   89, 1860,  390,  546, 1926,\n",
       "        1753,  497,  455,  839, 1926, 1433, 2305, 2702, 2743, 1158,  465,   37,\n",
       "         859, 1350, 1227, 2105, 1300,  118, 2530,  586, 2088, 1958,  564,   20,\n",
       "          87,  169, 1344,  984, 2517, 1724, 2448, 2702,  609, 1446, 2532,  594,\n",
       "          81, 1079, 2620, 1575, 1121, 1199, 1450, 2332,  704, 2547, 1832,  298,\n",
       "        1121, 2054, 1655, 2428, 1512, 1453, 1733, 2767,  746, 1657, 1093, 2423,\n",
       "        2635, 1121, 1958, 1494,  238, 1697, 1202,  788, 2100,  567, 1512, 2577,\n",
       "         905, 2355, 1433,  301, 1499, 2145, 1399, 1202, 2350, 1074, 2085,  801,\n",
       "         659,    5, 2356, 1629, 1738,  924,  908, 1005, 2248, 2387, 1010,  782,\n",
       "         400,  459,  124, 2306, 1349, 1202,  974, 1344, 2548, 2448, 2233,  435,\n",
       "        1287, 2357, 2445, 1886,  848, 1347, 1963, 1963, 1290,  703, 1480, 2355,\n",
       "         439,  221, 1222,  912,  835, 2037, 1220, 2031, 2325,  129, 2618, 2409,\n",
       "         155, 2660, 1067, 2660,  729, 1367, 1389, 2484, 2469,  290, 2547,  537,\n",
       "        2742, 1938, 2315, 2510, 1659, 1327,  538,  659, 2475, 2556,  311, 1220,\n",
       "         984,  155, 1009, 1783,  559, 1944, 2100,  678, 1121,  184,    3, 1860,\n",
       "        1244, 1560, 1944, 1738, 1591,  703, 2382,  561, 2559, 1192,  988,  212,\n",
       "        1431, 2484,  260,   55, 2444, 1741, 2209, 1738, 2424, 2660, 2318, 1926,\n",
       "        1341,  771,    6, 2155, 2439,  500,  884, 2448, 2252, 1065,  383,   95,\n",
       "        1134,  439, 2355, 1404,  328,  950, 2424,  326, 2627,   18, 2536, 1121,\n",
       "        2702, 2154, 1975,  813,  515,   40, 2102, 2051,  574, 1901, 1076, 2315,\n",
       "        1587,  746, 1110, 2702,  491, 2556, 2255, 2589, 1625, 1344,  991,  609,\n",
       "        1034, 1653,  713, 1214])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_dict_1['user_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval: HR = 0.1010, NDCG = 0.0470 [8.9 s]\n",
      "Epoch = 0\n",
      "Epoch completed 6.2 s\n",
      "Train Loss: 0.4724575046485903\n",
      "doing epoch 0\n",
      "Eval: HR = 0.4915, NDCG = 0.2594 [8.5 s]\n",
      "Epoch = 1\n",
      "Epoch completed 6.3 s\n",
      "Train Loss: 0.373765311114418\n",
      "doing epoch 1\n",
      "Eval: HR = 0.5677, NDCG = 0.3115 [8.6 s]\n",
      "Epoch = 2\n",
      "Epoch completed 6.2 s\n",
      "Train Loss: 0.3505717394550528\n",
      "doing epoch 2\n",
      "Eval: HR = 0.5753, NDCG = 0.3177 [8.5 s]\n",
      "Epoch = 3\n",
      "Epoch completed 6.2 s\n",
      "Train Loss: 0.3420019286677809\n",
      "doing epoch 3\n",
      "Eval: HR = 0.5758, NDCG = 0.3171 [8.6 s]\n",
      "Epoch = 4\n",
      "Epoch completed 6.2 s\n",
      "Train Loss: 0.33797582275763094\n",
      "doing epoch 4\n",
      "Eval: HR = 0.5765, NDCG = 0.3175 [8.5 s]\n",
      "Epoch = 5\n",
      "Epoch completed 6.1 s\n",
      "Train Loss: 0.3354666643670637\n",
      "doing epoch 5\n",
      "Eval: HR = 0.5775, NDCG = 0.3178 [8.6 s]\n",
      "Epoch = 6\n",
      "Epoch completed 6.2 s\n",
      "Train Loss: 0.33374034579061795\n",
      "doing epoch 6\n",
      "Eval: HR = 0.5750, NDCG = 0.3170 [8.5 s]\n",
      "Epoch = 7\n",
      "Epoch completed 6.1 s\n",
      "Train Loss: 0.33213773711185823\n",
      "doing epoch 7\n",
      "Eval: HR = 0.5737, NDCG = 0.3168 [8.5 s]\n",
      "Epoch = 8\n",
      "Epoch completed 6.2 s\n",
      "Train Loss: 0.33039561218162233\n",
      "doing epoch 8\n",
      "Eval: HR = 0.5718, NDCG = 0.3155 [8.4 s]\n",
      "Epoch = 9\n",
      "Epoch completed 6.0 s\n",
      "Train Loss: 0.3282326054613722\n",
      "doing epoch 9\n",
      "Eval: HR = 0.5719, NDCG = 0.3150 [8.3 s]\n",
      "Epoch = 10\n",
      "Epoch completed 6.1 s\n",
      "Train Loss: 0.32559337641042085\n",
      "doing epoch 10\n",
      "Eval: HR = 0.5671, NDCG = 0.3142 [8.6 s]\n",
      "Epoch = 11\n",
      "Epoch completed 6.5 s\n",
      "Train Loss: 0.3226649827276732\n",
      "doing epoch 11\n",
      "Eval: HR = 0.5698, NDCG = 0.3141 [8.6 s]\n",
      "Epoch = 12\n",
      "Epoch completed 6.2 s\n",
      "Train Loss: 0.31902143702275243\n",
      "doing epoch 12\n",
      "Eval: HR = 0.5655, NDCG = 0.3117 [8.6 s]\n",
      "Epoch = 13\n",
      "Epoch completed 6.1 s\n",
      "Train Loss: 0.31519468620868485\n",
      "doing epoch 13\n",
      "Eval: HR = 0.5614, NDCG = 0.3080 [8.6 s]\n",
      "Epoch = 14\n",
      "Epoch completed 6.4 s\n",
      "Train Loss: 0.3112526012202296\n",
      "doing epoch 14\n",
      "Eval: HR = 0.5619, NDCG = 0.3088 [8.6 s]\n",
      "Epoch = 15\n",
      "Epoch completed 6.4 s\n",
      "Train Loss: 0.30747753572335224\n",
      "doing epoch 15\n",
      "Eval: HR = 0.5596, NDCG = 0.3063 [8.6 s]\n",
      "Epoch = 16\n",
      "Epoch completed 6.4 s\n",
      "Train Loss: 0.3035279626514913\n",
      "doing epoch 16\n",
      "Eval: HR = 0.5583, NDCG = 0.3060 [8.7 s]\n",
      "Epoch = 17\n",
      "Epoch completed 6.4 s\n",
      "Train Loss: 0.3000247973976249\n",
      "doing epoch 17\n",
      "Eval: HR = 0.5541, NDCG = 0.3038 [8.7 s]\n",
      "Epoch = 18\n",
      "Epoch completed 6.4 s\n",
      "Train Loss: 0.2966124976287856\n",
      "doing epoch 18\n",
      "Eval: HR = 0.5515, NDCG = 0.3009 [8.7 s]\n",
      "Epoch = 19\n",
      "Epoch completed 6.4 s\n",
      "Train Loss: 0.2936680627147063\n",
      "doing epoch 19\n",
      "Eval: HR = 0.5501, NDCG = 0.2994 [8.7 s]\n",
      "Epoch = 20\n",
      "Epoch completed 6.4 s\n",
      "Train Loss: 0.2909131327886484\n",
      "doing epoch 20\n",
      "Eval: HR = 0.5478, NDCG = 0.2986 [8.7 s]\n",
      "Epoch = 21\n",
      "Epoch completed 6.4 s\n",
      "Train Loss: 0.2883447077783961\n",
      "doing epoch 21\n",
      "Eval: HR = 0.5481, NDCG = 0.2982 [8.7 s]\n",
      "Epoch = 22\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-8f123735cc7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     epoch_loss = train_one_epoch(model, training_data_generator,\n\u001b[0;32m---> 10\u001b[0;31m                                  loss_fn, optimizer, epoch, device)\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"doing epoch {epoch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/encodings_for_cold_start/src/utils.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, data_loader, loss_fn, optimizer, epoch_no, device, verbose)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;31m# accumulate the loss for monitoring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mepoch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/spike_basicoV5/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Check Init performance\n",
    "hr, ndcg = test(model, full_dataset, topK)\n",
    "hr_list.append(hr)\n",
    "ndcg_list.append(ndcg)\n",
    "BCE_loss_list.append(1)\n",
    "# do the epochs now\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = train_one_epoch(model, training_data_generator,\n",
    "                                 loss_fn, optimizer, epoch, device)\n",
    "\n",
    "    if epoch % verbose == 0:\n",
    "        hr, ndcg = test(model, full_dataset, topK)\n",
    "        hr_list.append(hr)\n",
    "        ndcg_list.append(ndcg)\n",
    "        BCE_loss_list.append(epoch_loss)\n",
    "        # if hr > best_hr:\n",
    "        #     best_hr, best_ndcg, best_iter = hr, ndcg, epoch\n",
    "        #     if args.out > 0:\n",
    "        #         model.save(model_out_file, overwrite=True)\n",
    "print(\"hr for epochs: \", hr_list)\n",
    "print(\"ndcg for epochs: \", ndcg_list)\n",
    "print(\"loss for epochs: \", BCE_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 % verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spike_basicoV5]",
   "language": "python",
   "name": "conda-env-spike_basicoV5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
