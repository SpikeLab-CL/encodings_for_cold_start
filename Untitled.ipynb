{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All datasets are subclasses of torchtext.data.Dataset,\n",
    "which inherits from torch.utils.data.Dataset i.e, they have split and iters methods implemented.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext import datasets\n",
    "from torchtext.data import Field\n",
    "from torchtext.vocab import GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_iter, test_iter = datasets.IMDB.iters(batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(lower=True, include_lengths=True, batch_first=True)\n",
    "LABEL = Field(sequential=False)\n",
    "\n",
    "# make splits for data\n",
    "train, test = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "# build the vocabulary\n",
    "TEXT.build_vocab(train, vectors=GloVe(name='6B', dim=300))\n",
    "LABEL.build_vocab(train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    }
   ],
   "source": [
    "# make iterator for splits\n",
    "train_iter, test_iter = torchtext.data.BucketIterator.splits(\n",
    "    (train, test), batch_size=3, device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Practical Torchtext\n",
    "\n",
    "https://github.com/keitakurita/practical-torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.csv  train.csv  valid.csv\n"
     ]
    }
   ],
   "source": [
    "!ls data/practical_pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchtext.data import TabularDataset\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "data_folder = \"data/practical_pytorch/\"\n",
    "\n",
    "pd.read_csv(data_folder + \"train.csv\").head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000eefc67a2c930f</td>\n",
       "      <td>Radial symmetry \\n\\nSeveral now extinct lineag...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000f35deef84dc4a</td>\n",
       "      <td>There's no need to apologize. A Wikipedia arti...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  000eefc67a2c930f  Radial symmetry \\n\\nSeveral now extinct lineag...      0   \n",
       "1  000f35deef84dc4a  There's no need to apologize. A Wikipedia arti...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(data_folder + \"valid.csv\").head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field\n",
    "tokenize = lambda x: x.split()\n",
    "TEXT = Field(sequential=True, tokenize=tokenize, lower=True)\n",
    "LABEL = Field(sequential=False, use_vocab=False)\n",
    "\n",
    "tv_datafields = [(\"id\", None), # we won't be needing the id, \n",
    "                              # so we pass in None as the field\n",
    "                 (\"comment_text\", TEXT), (\"toxic\", LABEL),\n",
    "                 (\"severe_toxic\", LABEL), (\"threat\", LABEL),\n",
    "                 (\"obscene\", LABEL), (\"insult\", LABEL),\n",
    "                 (\"identity_hate\", LABEL)]\n",
    "\n",
    "trn, vld = TabularDataset.splits(\n",
    "        path=data_folder, \n",
    "        train='train.csv', validation=\"valid.csv\",\n",
    "        format='csv',\n",
    "        skip_header=True,\n",
    "        fields=tv_datafields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_datafields = [(\"id\", None), \n",
    "                 (\"comment_text\", TEXT)\n",
    "]\n",
    "\n",
    "tst = TabularDataset(\n",
    "        path= data_folder + \"test.csv\", # the file path\n",
    "        format='csv',\n",
    "        skip_header=True, \n",
    "        fields=tst_datafields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(trn) #(trx, vectors=GloVe(name='6B', dim=300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 78),\n",
       " ('to', 41),\n",
       " ('you', 33),\n",
       " ('of', 30),\n",
       " ('and', 26),\n",
       " ('a', 26),\n",
       " ('is', 24),\n",
       " ('that', 22),\n",
       " ('i', 20),\n",
       " ('if', 19)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['comment_text', 'toxic', 'severe_toxic', 'threat', 'obscene', 'insult', 'identity_hate'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[0].__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['explanation', 'why', 'the', 'edits', 'made']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[0].comment_text[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter = BucketIterator.splits(\n",
    "        (trn, vld),\n",
    "        batch_sizes=(64, 64),\n",
    "        device=device,\n",
    "        sort_key=lambda x: len(x.comment_text), # the BucketIterator needs to be told what function it should use to group the data.\n",
    "        sort_within_batch=False,\n",
    "        repeat=False # we pass repeat=False because we want to wrap this Iterator layer.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 25]\n",
       "\t[.comment_text]:[torch.cuda.LongTensor of size 494x25 (GPU 0)]\n",
       "\t[.toxic]:[torch.cuda.LongTensor of size 25 (GPU 0)]\n",
       "\t[.severe_toxic]:[torch.cuda.LongTensor of size 25 (GPU 0)]\n",
       "\t[.threat]:[torch.cuda.LongTensor of size 25 (GPU 0)]\n",
       "\t[.obscene]:[torch.cuda.LongTensor of size 25 (GPU 0)]\n",
       "\t[.insult]:[torch.cuda.LongTensor of size 25 (GPU 0)]\n",
       "\t[.identity_hate]:[torch.cuda.LongTensor of size 25 (GPU 0)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(train_iter.__iter__()); batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['batch_size', 'dataset', 'fields', 'input_fields', 'target_fields', 'comment_text', 'toxic', 'severe_toxic', 'threat', 'obscene', 'insult', 'identity_hate'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = Iterator(tst, batch_size=64, device=device, sort=False,\n",
    "                     sort_within_batch=False, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchWrapper:\n",
    "    def __init__(self, dl, x_var, y_vars):\n",
    "        self.dl, self.x_var, self.y_vars = dl, x_var, y_vars\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            x = getattr(batch, self.x_var) # we assume only one input in this wrapper\n",
    "            \n",
    "            if self.y_vars is not None: # we will concatenate y into a single tensor\n",
    "                y = torch.cat([getattr(batch, feat).unsqueeze(1) for feat in self.y_vars], dim=1).float()\n",
    "            else:\n",
    "                y = torch.zeros((1))\n",
    "\n",
    "            yield (x, y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "    \n",
    "\n",
    "train_dl = BatchWrapper(train_iter, \"comment_text\", \n",
    "                        [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\",\n",
    "                         \"insult\", \"identity_hate\"])\n",
    "valid_dl = BatchWrapper(val_iter, \"comment_text\", [\"toxic\", \"severe_toxic\",\n",
    "                                                   \"obscene\", \"threat\", \"insult\",\n",
    "                                                   \"identity_hate\"])\n",
    "test_dl = BatchWrapper(test_iter, \"comment_text\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleBiLSTMBaseline(\n",
       "  (embedding): Embedding(784, 100)\n",
       "  (encoder): LSTM(100, 500, dropout=0.1)\n",
       "  (linear_layers): ModuleList()\n",
       "  (predictor): Linear(in_features=500, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class SimpleBiLSTMBaseline(nn.Module):\n",
    "    def __init__(self, hidden_dim, emb_dim=300,\n",
    "                 spatial_dropout=0.05, recurrent_dropout=0.1, num_linear=1):\n",
    "        super().__init__() \n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab), emb_dim)\n",
    "        self.encoder = nn.LSTM(emb_dim, hidden_dim, num_layers=1,\n",
    "                               dropout=recurrent_dropout)\n",
    "        self.linear_layers = []\n",
    "        for _ in range(num_linear - 1):\n",
    "            self.linear_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.linear_layers = nn.ModuleList(self.linear_layers)\n",
    "        self.predictor = nn.Linear(hidden_dim, 6)\n",
    "    \n",
    "    def forward(self, seq):\n",
    "        hdn, _ = self.encoder(self.embedding(seq))\n",
    "        feature = hdn[-1, :, :]\n",
    "        for layer in self.linear_layers:\n",
    "            feature = layer(feature)\n",
    "        preds = self.predictor(feature)\n",
    "        return preds\n",
    "    \n",
    "\n",
    "em_sz = 100\n",
    "nh = 500\n",
    "nl = 3\n",
    "model = SimpleBiLSTMBaseline(nh, emb_dim=em_sz)\n",
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.67it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.87it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 19.7675, Validation Loss: 2.0425\n",
      "Epoch: 2, Training Loss: 19.7676, Validation Loss: 1.9519\n",
      "Epoch: 3, Training Loss: 19.7658, Validation Loss: 2.1259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 15.22it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.71it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.15it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss: 19.7660, Validation Loss: 2.1997\n",
      "Epoch: 5, Training Loss: 19.7661, Validation Loss: 2.2132\n",
      "Epoch: 6, Training Loss: 19.7661, Validation Loss: 2.1882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.34it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.53it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Training Loss: 19.7659, Validation Loss: 2.1762\n",
      "Epoch: 8, Training Loss: 19.7658, Validation Loss: 2.1911\n",
      "Epoch: 9, Training Loss: 19.7658, Validation Loss: 2.2463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.62it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.36it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Training Loss: 19.7658, Validation Loss: 2.3242\n",
      "Epoch: 11, Training Loss: 19.7658, Validation Loss: 2.3830\n",
      "Epoch: 12, Training Loss: 19.7658, Validation Loss: 2.3815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.59it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.59it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Training Loss: 19.7658, Validation Loss: 2.3661\n",
      "Epoch: 14, Training Loss: 19.7657, Validation Loss: 2.4545\n",
      "Epoch: 15, Training Loss: 19.7655, Validation Loss: 2.4653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 17.08it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.34it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.65it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Training Loss: 19.7650, Validation Loss: 2.5071\n",
      "Epoch: 17, Training Loss: 19.7643, Validation Loss: 2.5903\n",
      "Epoch: 18, Training Loss: 19.7635, Validation Loss: 2.6039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.53it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.48it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Training Loss: 19.7626, Validation Loss: 2.6680\n",
      "Epoch: 20, Training Loss: 19.7624, Validation Loss: 2.8529\n",
      "Epoch: 21, Training Loss: 19.7622, Validation Loss: 2.4607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 17.55it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.49it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.30it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Training Loss: 19.7619, Validation Loss: 2.6596\n",
      "Epoch: 23, Training Loss: 19.7616, Validation Loss: 2.8700\n",
      "Epoch: 24, Training Loss: 19.7617, Validation Loss: 3.0955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 17.19it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.38it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.22it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Training Loss: 19.7611, Validation Loss: 3.1806\n",
      "Epoch: 26, Training Loss: 19.7610, Validation Loss: 3.2209\n",
      "Epoch: 27, Training Loss: 19.7608, Validation Loss: 3.3348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 17.40it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.01it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Training Loss: 19.7606, Validation Loss: 3.4046\n",
      "Epoch: 29, Training Loss: 19.7605, Validation Loss: 3.5304\n",
      "Epoch: 30, Training Loss: 19.7605, Validation Loss: 3.6833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    model.train() # turn on training mode\n",
    "    for x, y in tqdm.tqdm(train_dl): # thanks to our wrapper, we can intuitively iterate over our data!\n",
    "        opt.zero_grad()\n",
    "\n",
    "        preds = model(x)\n",
    "        loss = loss_func(preds, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        running_loss += loss.item() + x.size(0)#loss.data[0] * x.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / len(trn)\n",
    "    \n",
    "    # calculate the validation loss for this epoch\n",
    "    val_loss = 0.0\n",
    "    model.eval() # turn on evaluation mode\n",
    "    for x, y in valid_dl:\n",
    "        preds = model(x)\n",
    "        loss = loss_func(preds, y)\n",
    "        val_loss += loss.item() * x.size(0)\n",
    "\n",
    "    val_loss /= len(vld)\n",
    "    print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, epoch_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spike_basicoV5]",
   "language": "python",
   "name": "conda-env-spike_basicoV5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
